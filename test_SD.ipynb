{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d57fb4ec-913f-4b91-bfab-80fb5cd66be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8a4f99d-689c-47e2-95aa-72476a72ea4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m279.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in ./.miniconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.miniconda3/lib/python3.11/site-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.miniconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.7.24-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in ./.miniconda3/lib/python3.11/site-packages (from transformers) (2.29.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.miniconda3/lib/python3.11/site-packages (from transformers) (4.51.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.miniconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.miniconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.miniconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.miniconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.miniconda3/lib/python3.11/site-packages (from requests->transformers) (2023.5.7)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.7.24-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (786 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m786.6/786.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.4/435.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, pyyaml, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed fsspec-2024.6.1 huggingface-hub-0.24.6 pyyaml-6.0.2 regex-2024.7.24 safetensors-0.4.4 tokenizers-0.19.1 transformers-4.44.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15d51c62-8a94-4e7b-b0f5-f6ce958480ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f6b16ba-be9f-4a19-b854-c62e9a7a6346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d29feaf02342e990dbee3f20840e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='tokenizer_config.json'), FloatProgress(value=0.0, max=1868.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd81ce6b13e94b7281f773088a8f38cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='tokenizer.json'), FloatProgress(value=0.0, max=1367962.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3c3dfc21f24328a3819d9ed90dda0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='config.json'), FloatProgress(value=0.0, max=760.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ccc76ce3c141db8f2923d27a5da8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='model.safetensors.index.json'), FloatProgress(value=0.0, max=25125.0), HTML(value='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b4d3185dad46a8b259b8179949e84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading shards'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbce1696b7e4ff3a1c7f017c99b446f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='model-00001-of-00002.safetensors'), FloatProgress(value=0.0, max=9978667672.0), HTM…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8f19c533b443acb605b181aa35649f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='model-00002-of-00002.safetensors'), FloatProgress(value=0.0, max=3502391696.0), HTM…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2dc395dc4b473888b4af0c6f3b4017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Loading checkpoint shards'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a8f85986b14a78ab355eb20127f997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='generation_config.json'), FloatProgress(value=0.0, max=119.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0fcddb7-36ed-4d1a-9bdf-71575f4c5d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f745f95a-22b8-4d68-b492-1fb1d8775ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vanilla_edit(prompt: str, max_tokens: int) -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'], \n",
    "        max_length=len(inputs['input_ids'][0]) + max_tokens, \n",
    "        do_sample=False,  # greedy sampling\n",
    "        temperature=0.0\n",
    "    )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e9f7e8e9-0b73-42d1-b90d-b52d3695b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "````txt\n",
    "Please add a single comment in code at return statement\n",
    "\n",
    "```py\n",
    "def return_tensors(input:str):\n",
    "    return torch.randn((2,4))\n",
    "```\n",
    "\n",
    "```py\n",
    "````\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a7f06ae-82d0-44ab-9dfe-2594595e3a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "draft=\"\"\"\n",
    "Review the code below and add a comment on the return statement if it is missing:\n",
    "\n",
    "```py\n",
    "def return_tensors(input: str):\n",
    "    return torch.randn((2, 4))\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6db5ed98-f4dd-4cf1-9920-487238bc1dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Prompt: tensor([[32013,   185,  4686,  4686,  9313,   185,  7912,   957,   245,  2805,\n",
      "          5273,   279,  2974,   429,   967,  6158,   185,   185, 10252,  4016,\n",
      "           185,  1551,   967,    62,    83,   657,   710,     7,  3584,    25,\n",
      "          2006,  1772,   185,   315,   967,  6465,   358,    13, 21035,    77,\n",
      "          5930,    17,    11,    19,  1435,   185, 10252,   185,   185, 10252,\n",
      "          4016,   185,  4686,  4686,   185]], device='cuda:0')\n",
      "Tokenized Draft: tensor([[32013,   185, 20909,   254,  2974,  2867,   285,   957,   245,  5273,\n",
      "           331,   254,   967,  6158,   562,   359,   317,  7088,    25,   185,\n",
      "           185, 10252,  4016,   185,  1551,   967,    62,    83,   657,   710,\n",
      "             7,  3584,    25,  1401,  1772,   185,   315,   967,  6465,   358,\n",
      "            13, 21035,    77,  5930,    17,    11,   207,    19,  1435,   185,\n",
      "         10252,   185]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "prompt_inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "draft_inputs = tokenizer(draft, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(f\"Tokenized Prompt: {prompt_inputs['input_ids']}\")\n",
    "print(f\"Tokenized Draft: {draft_inputs['input_ids']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "faffb069-566e-4585-9916-5bac835a8c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_inputs = tokenizer(draft, return_tensors=\"pt\").to(device)\n",
    "draft_tokens = draft_inputs['input_ids'][0].to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8fab940f-1bdd-4324-8116-24580845300f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Draft Tokens: tensor([32013,   185, 20909,   254,  2974,  2867,   285,   957,   245,  5273,\n",
      "          331,   254,   967,  6158,   562,   359,   317,  7088,    25,   185,\n",
      "          185, 10252,  4016,   185,  1551,   967,    62,    83,   657,   710,\n",
      "            7,  3584,    25,  1401,  1772,   185,   315,   967,  6465,   358,\n",
      "           13, 21035,    77,  5930,    17,    11,   207,    19,  1435,   185,\n",
      "        10252,   185], device='cuda:0')\n",
      "Cloned Draft Tokens: tensor([32013,   185, 20909,   254,  2974,  2867,   285,   957,   245,  5273,\n",
      "          331,   254,   967,  6158,   562,   359,   317,  7088,    25,   185,\n",
      "          185, 10252,  4016,   185,  1551,   967,    62,    83,   657,   710,\n",
      "            7,  3584,    25,  1401,  1772,   185,   315,   967,  6465,   358,\n",
      "           13, 21035,    77,  5930,    17,    11,   207,    19,  1435,   185,\n",
      "        10252,   185], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "generated_tokens = draft_tokens.clone().detach()\n",
    "\n",
    "print(f\"Original Draft Tokens: {draft_tokens}\")\n",
    "print(f\"Cloned Draft Tokens: {generated_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78a22eba-7688-4fd3-97cc-63574d79b5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 0: Predicted Token ID: 185, Draft Token ID: 32013\n",
      "Token 1: Predicted Token ID: 315, Draft Token ID: 185\n",
      "Token 2: Predicted Token ID: 4686, Draft Token ID: 20909\n",
      "Token 3: Predicted Token ID: 185, Draft Token ID: 254\n",
      "Token 4: Predicted Token ID: 185, Draft Token ID: 2974\n",
      "Token 5: Predicted Token ID: 185, Draft Token ID: 2867\n",
      "Token 6: Predicted Token ID: 5213, Draft Token ID: 285\n",
      "Token 7: Predicted Token ID: 254, Draft Token ID: 957\n",
      "Token 8: Predicted Token ID: 6413, Draft Token ID: 245\n",
      "Token 9: Predicted Token ID: 1348, Draft Token ID: 5273\n",
      "Token 10: Predicted Token ID: 276, Draft Token ID: 331\n",
      "Token 11: Predicted Token ID: 254, Draft Token ID: 254\n",
      "Token 12: Predicted Token ID: 276, Draft Token ID: 967\n",
      "Token 13: Predicted Token ID: 254, Draft Token ID: 6158\n",
      "Token 14: Predicted Token ID: 6158, Draft Token ID: 562\n",
      "Token 15: Predicted Token ID: 13, Draft Token ID: 359\n",
      "Token 16: Predicted Token ID: 4686, Draft Token ID: 317\n",
      "Token 17: Predicted Token ID: 10252, Draft Token ID: 7088\n",
      "Token 18: Predicted Token ID: 11364, Draft Token ID: 25\n",
      "Token 19: Predicted Token ID: 185, Draft Token ID: 185\n",
      "Token 20: Predicted Token ID: 1551, Draft Token ID: 185\n",
      "Token 21: Predicted Token ID: 1155, Draft Token ID: 10252\n",
      "Token 22: Predicted Token ID: 62, Draft Token ID: 4016\n",
      "Token 23: Predicted Token ID: 26053, Draft Token ID: 185\n",
      "Token 24: Predicted Token ID: 14563, Draft Token ID: 1551\n",
      "Token 25: Predicted Token ID: 62, Draft Token ID: 967\n",
      "Token 26: Predicted Token ID: 7, Draft Token ID: 62\n",
      "Token 27: Predicted Token ID: 87, Draft Token ID: 83\n",
      "Token 28: Predicted Token ID: 62, Draft Token ID: 657\n",
      "Token 29: Predicted Token ID: 6465, Draft Token ID: 710\n",
      "Token 30: Predicted Token ID: 1772, Draft Token ID: 7\n",
      "Token 31: Predicted Token ID: 185, Draft Token ID: 3584\n",
      "Token 32: Predicted Token ID: 315, Draft Token ID: 25\n",
      "Token 33: Predicted Token ID: 967, Draft Token ID: 1401\n",
      "Token 34: Predicted Token ID: 2773, Draft Token ID: 1772\n",
      "Token 35: Predicted Token ID: 358, Draft Token ID: 185\n",
      "Token 36: Predicted Token ID: 13, Draft Token ID: 315\n",
      "Token 37: Predicted Token ID: 23046, Draft Token ID: 967\n",
      "Token 38: Predicted Token ID: 77, Draft Token ID: 6465\n",
      "Token 39: Predicted Token ID: 7, Draft Token ID: 358\n",
      "Token 40: Predicted Token ID: 16, Draft Token ID: 13\n",
      "Token 41: Predicted Token ID: 11, Draft Token ID: 21035\n",
      "Token 42: Predicted Token ID: 17, Draft Token ID: 77\n",
      "Token 43: Predicted Token ID: 1435, Draft Token ID: 5930\n",
      "Token 44: Predicted Token ID: 1494, Draft Token ID: 17\n",
      "Token 45: Predicted Token ID: 10252, Draft Token ID: 11\n",
      "Token 46: Predicted Token ID: 185, Draft Token ID: 207\n",
      "Token 47: Predicted Token ID: 4686, Draft Token ID: 19\n",
      "Token 48: Predicted Token ID: 10252, Draft Token ID: 1435\n",
      "Token 49: Predicted Token ID: 4016, Draft Token ID: 185\n",
      "Token 50: Predicted Token ID: 185, Draft Token ID: 10252\n",
      "Token 51: Predicted Token ID: 1551, Draft Token ID: 185\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx in range(len(draft_tokens)):\n",
    "    outputs = model(prompt_inputs['input_ids'])[0].to(device)\n",
    "    predicted_token_id = torch.argmax(outputs[:, idx, :], dim=-1).item()\n",
    "    \n",
    "    print(f\"Token {idx}: Predicted Token ID: {predicted_token_id}, Draft Token ID: {draft_tokens[idx].item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "16282f6a-87df-4493-8989-525423f3b0c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 0. Generated text: \n",
      "    public function getId(): ?int\n",
      "    {\n",
      "        return $this->id;\n",
      "    }\n",
      "\n",
      "    public function getName(): ?string\n",
      "    {\n",
      "        return $this->name;\n",
      "    }\n",
      "\n",
      "    public function setName(string $name): self\n",
      "    {\n",
      "        $this->name = $name;\n",
      "\n",
      "        return $this;\n",
      "    }\n",
      "\n",
      "    public function getDescription(): ?string\n",
      "    {\n",
      "        return $this->description;\n",
      "    }\n",
      "\n",
      "    public function setDescription(string $description): self\n",
      "    {\n",
      "        $this->description = $description;\n",
      "\n",
      "        return $this;\n",
      "    }\n",
      "\n",
      "    public function getPrice(): ?float\n",
      "\n",
      "    public function getId(): ?int\n",
      "    {\n",
      "        return $this->id;\n",
      "    }\n",
      "\n",
      "    public function getName(): ?string\n",
      "    {\n",
      "        return $this->name;\n",
      "    }\n",
      "\n",
      "    public function setName(string $name): self\n",
      "    {\n",
      "        $this->name = $name;\n",
      "\n",
      "        return $this;\n",
      "    }\n",
      "\n",
      "    public function getDescription(): ?string\n",
      "    {\n",
      "        return $this->description;\n",
      "    }\n",
      "\n",
      "    public function setDescription(string $description): self\n",
      "    {\n",
      "        $this->description = $description;\n",
      "\n",
      "        return $this;\n",
      "    }\n",
      "\n",
      "    public function getPrice(): ?float\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 1. Generated text: \n",
      "    public function getId(): ?int\n",
      "    {\n",
      "        return $this->id;\n",
      "    }\n",
      "\n",
      "    public function getName(): ?string\n",
      "    {\n",
      "        return $this->name;\n",
      "    }\n",
      "\n",
      "    public function setName(string $name): self\n",
      "    {\n",
      "        $this->name = $name;\n",
      "\n",
      "        return $this;\n",
      "    }\n",
      "\n",
      "    public function getDescription(): ?string\n",
      "    {\n",
      "        return $this->description;\n",
      "    }\n",
      "\n",
      "    public function setDescription(string $description): self\n",
      "    {\n",
      "        $this->description = $description;\n",
      "\n",
      "        return $this;\n",
      "    }\n",
      "\n",
      "    public function getPrice(): ?float\n",
      "\n",
      "    public function getId(): ?int\n",
      "    {\n",
      "        return $this->id;\n",
      "    }\n",
      "\n",
      "    public function getName(): ?string\n",
      "    {\n",
      "        return $this->name;\n",
      "    }\n",
      "\n",
      "    public function setName(string $name): self\n",
      "    {\n",
      "        $this->name = $name;\n",
      "\n",
      "        return $this;\n",
      "    }\n",
      "\n",
      "    public function getDescription(): ?string\n",
      "    {\n",
      "        return $this->description;\n",
      "    }\n",
      "\n",
      "    public function setDescription(string $description): self\n",
      "    {\n",
      "        $this->description = $description;\n",
      "\n",
      "        return $this;\n",
      "    }\n",
      "\n",
      "    public function getPrice(): ?float\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 2. Generated text: \n",
      "````\n",
      "\n",
      "## 2. 정렬 알고리즘\n",
      "\n",
      "### 2.1 선택 정렬\n",
      "\n",
      "- 선택 정렬은 배열을 정렬하는 방법으로, 전체 배열을 순회하며 가장 작은 값을 \n",
      "\n",
      "````\n",
      "\n",
      "## 2. 정렬 알고리즘\n",
      "\n",
      "### 2.1 선택 정렬\n",
      "\n",
      "- 선택 정렬은 배열을 정렬하는 방법으로, 전체 배열을 순회하며 가장 작은 값을 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 3. Generated text: \n",
      "````\n",
      "\n",
      "## 2. 정렬 알고리즘\n",
      "\n",
      "### 2.1 선택 정렬\n",
      "\n",
      "- 선택 정렬은 배열을 정렬하는 방법으로, 전체 배열을 순회하며 가장 작은 값을 \n",
      "\n",
      "````\n",
      "\n",
      "## 2. 정렬 알고리즘\n",
      "\n",
      "### 2.1 선택 정렬\n",
      "\n",
      "- 선택 정렬은 배열을 정렬하는 방법으로, 전체 배열을 순회하며 가장 작은 값을 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 4. Generated text: \n",
      "````txt\n",
      "\n",
      "````\n",
      "\n",
      "## 参考资料\n",
      "\n",
      "- [GitHub - jupyter/docker-stacks: Docker images for Jupyter](https://github.com/jupyter/docker-stacks)\n",
      "- [Docker Hub - jupyter/scipy-notebook: Jupyter Notebook Image](https://hub.docker.com/r/jupyter/scipy-notebook/)\n",
      "- [Docker Hub - jupyter/datascience-notebook: Jupyter Notebook Image](https://hub.docker.com/r/jupyter/datascience-\n",
      "\n",
      "````txt\n",
      "\n",
      "````\n",
      "\n",
      "## 参考资料\n",
      "\n",
      "- [GitHub - jupyter/docker-stacks: Docker images for Jupyter](https://github.com/jupyter/docker-stacks)\n",
      "- [Docker Hub - jupyter/scipy-notebook: Jupyter Notebook Image](https://hub.docker.com/r/jupyter/scipy-notebook/)\n",
      "- [Docker Hub - jupyter/datascience-notebook: Jupyter Notebook Image](https://hub.docker.com/r/jupyter/datascience-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 5. Generated text: \n",
      "````txt\n",
      "\n",
      "````\n",
      "\n",
      "## 参考资料\n",
      "\n",
      "- [GitHub - jupyter/docker-stacks: Docker images for Jupyter](https://github.com/jupyter/docker-stacks)\n",
      "- [Docker Hub - jupyter/scipy-notebook: Jupyter Notebook Image](https://hub.docker.com/r/jupyter/scipy-notebook/)\n",
      "- [Docker Hub - jupyter/datascience-notebook: Jupyter Notebook Image](https://hub.docker.com/r/jupyter/datascience-\n",
      "\n",
      "````txt\n",
      "\n",
      "````\n",
      "\n",
      "## 参考资料\n",
      "\n",
      "- [GitHub - jupyter/docker-stacks: Docker images for Jupyter](https://github.com/jupyter/docker-stacks)\n",
      "- [Docker Hub - jupyter/scipy-notebook: Jupyter Notebook Image](https://hub.docker.com/r/jupyter/scipy-notebook/)\n",
      "- [Docker Hub - jupyter/datascience-notebook: Jupyter Notebook Image](https://hub.docker.com/r/jupyter/datascience-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 6. Generated text: \n",
      "````txt\n",
      "Please enter the number of elements in the array: 5\n",
      "Please enter the elements of the array: 1 2 3 4 5\n",
      "The array is: [1, 2, 3, 4, 5]\n",
      "The reversed array is: [5, 4, 3, 2, 1]\n",
      "````\n",
      "\n",
      "## 2. 문제 설명\n",
      "\n",
      "배열을 입력받아 배열의 원소를 역순으�\n",
      "\n",
      "````txt\n",
      "Please enter the number of elements in the array: 5\n",
      "Please enter the elements of the array: 1 2 3 4 5\n",
      "The array is: [1, 2, 3, 4, 5]\n",
      "The reversed array is: [5, 4, 3, 2, 1]\n",
      "````\n",
      "\n",
      "## 2. 문제 설명\n",
      "\n",
      "배열을 입력받아 배열의 원소를 역순으�\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 7. Generated text: \n",
      "````txt\n",
      "Please add the following to your .env file:\n",
      "\n",
      "DATABASE_URL=postgresql://user:password@localhost:5432/mydatabase\n",
      "````\n",
      "\n",
      "Then, you can use the `DATABASE_URL` environment variable to connect to your database.\n",
      "\n",
      "## Running the app\n",
      "\n",
      "To run the app, use the following command:\n",
      "\n",
      "```bash\n",
      "npm start\n",
      "```\n",
      "\n",
      "## Testing\n",
      "\n",
      "To run the tests, use the following command:\n",
      "\n",
      "```bash\n",
      "npm test\n",
      "```\n",
      "\n",
      "## Contributing\n",
      "\n",
      "Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\n",
      "\n",
      "Please\n",
      "\n",
      "````txt\n",
      "Please add the following to your .env file:\n",
      "\n",
      "DATABASE_URL=postgresql://user:password@localhost:5432/mydatabase\n",
      "````\n",
      "\n",
      "Then, you can use the `DATABASE_URL` environment variable to connect to your database.\n",
      "\n",
      "## Running the app\n",
      "\n",
      "To run the app, use the following command:\n",
      "\n",
      "```bash\n",
      "npm start\n",
      "```\n",
      "\n",
      "## Testing\n",
      "\n",
      "To run the tests, use the following command:\n",
      "\n",
      "```bash\n",
      "npm test\n",
      "```\n",
      "\n",
      "## Contributing\n",
      "\n",
      "Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\n",
      "\n",
      "Please\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 8. Generated text: \n",
      "````txt\n",
      "Please add a description of the changes you made here to help others understand your\n",
      "motivation and context. List any dependencies that are required for this change.\n",
      "\n",
      "Fixes # (issue)\n",
      "````\n",
      "\n",
      "## Type of change\n",
      "\n",
      "Please delete options that are not relevant.\n",
      "\n",
      "- [ ] Bug fix (non-breaking change which fixes an issue)\n",
      "- [ ] New feature (non-breaking change which adds functionality)\n",
      "- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\n",
      "- [ ] This change requires a documentation update\n",
      "\n",
      "## Checklist:\n",
      "\n",
      "- [ ] My code follows the style guidelines of this project\n",
      "- [ ] I\n",
      "\n",
      "````txt\n",
      "Please add a description of the changes you made here to help others understand your\n",
      "motivation and context. List any dependencies that are required for this change.\n",
      "\n",
      "Fixes # (issue)\n",
      "````\n",
      "\n",
      "## Type of change\n",
      "\n",
      "Please delete options that are not relevant.\n",
      "\n",
      "- [ ] Bug fix (non-breaking change which fixes an issue)\n",
      "- [ ] New feature (non-breaking change which adds functionality)\n",
      "- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)\n",
      "- [ ] This change requires a documentation update\n",
      "\n",
      "## Checklist:\n",
      "\n",
      "- [ ] My code follows the style guidelines of this project\n",
      "- [ ] I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 9. Generated text: \n",
      "````txt\n",
      "Please add a single line of text to the file.\n",
      "````\n",
      "\n",
      "The file should now contain:\n",
      "\n",
      "````txt\n",
      "Please add a single line of text to the file.\n",
      "````\n",
      "\n",
      "## Task 2: Append text to a file\n",
      "\n",
      "Now, append the following text to the file:\n",
      "\n",
      "````txt\n",
      "This text is being appended to the file.\n",
      "````\n",
      "\n",
      "The file should now contain:\n",
      "\n",
      "````txt\n",
      "Please add a single line of text to the file.\n",
      "This text is being appended to the file.\n",
      "````\n",
      "\n",
      "## Task 3: Read the file\n",
      "\n",
      "Read the file and print its content to the console.\n",
      "\n",
      "\n",
      "````txt\n",
      "Please add a single line of text to the file.\n",
      "````\n",
      "\n",
      "The file should now contain:\n",
      "\n",
      "````txt\n",
      "Please add a single line of text to the file.\n",
      "````\n",
      "\n",
      "## Task 2: Append text to a file\n",
      "\n",
      "Now, append the following text to the file:\n",
      "\n",
      "````txt\n",
      "This text is being appended to the file.\n",
      "````\n",
      "\n",
      "The file should now contain:\n",
      "\n",
      "````txt\n",
      "Please add a single line of text to the file.\n",
      "This text is being appended to the file.\n",
      "````\n",
      "\n",
      "## Task 3: Read the file\n",
      "\n",
      "Read the file and print its content to the console.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 10. Generated text: \n",
      "````txt\n",
      "Please add a single comment to the end of the file.\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```python\n",
      "# Please add a single comment to the end of the file.\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "A comment in Python starts with a `#` symbol. The rest of the line is ignored by the Python interpreter. This is useful for adding notes or explanations to your code.\n",
      "\n",
      "In this case, the comment is added at the end of the file, after the code. It's a single line comment, but you could also use multi-line comments, which are enclosed in `'''` or `\"\"\"`.\n",
      "\n",
      "The solution is a single line comment that doesn\n",
      "\n",
      "````txt\n",
      "Please add a single comment to the end of the file.\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```python\n",
      "# Please add a single comment to the end of the file.\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "A comment in Python starts with a `#` symbol. The rest of the line is ignored by the Python interpreter. This is useful for adding notes or explanations to your code.\n",
      "\n",
      "In this case, the comment is added at the end of the file, after the code. It's a single line comment, but you could also use multi-line comments, which are enclosed in `'''` or `\"\"\"`.\n",
      "\n",
      "The solution is a single line comment that doesn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 12. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code to explain the purpose of the code.\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```python\n",
      "# This program calculates the area of a rectangle\n",
      "\n",
      "# Get the length and width from the user\n",
      "length = float(input(\"Enter the length of the rectangle: \"))\n",
      "width = float(input(\"Enter the width of the rectangle: \"))\n",
      "\n",
      "# Calculate the area\n",
      "area = length * width\n",
      "\n",
      "# Print the area\n",
      "print(\"The area of the rectangle is\", area)\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "This program is a simple calculator for the area of a rectangle. It first asks the user to input the length and width of the\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code to explain the purpose of the code.\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```python\n",
      "# This program calculates the area of a rectangle\n",
      "\n",
      "# Get the length and width from the user\n",
      "length = float(input(\"Enter the length of the rectangle: \"))\n",
      "width = float(input(\"Enter the width of the rectangle: \"))\n",
      "\n",
      "# Calculate the area\n",
      "area = length * width\n",
      "\n",
      "# Print the area\n",
      "print(\"The area of the rectangle is\", area)\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "This program is a simple calculator for the area of a rectangle. It first asks the user to input the length and width of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 13. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at the end of the file explaining what the code does.\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```python\n",
      "# This program calculates the area of a rectangle\n",
      "\n",
      "# Function to calculate area\n",
      "def calculate_area(length, width):\n",
      "    return length * width\n",
      "\n",
      "# Input length and width\n",
      "length = float(input(\"Enter the length of the rectangle: \"))\n",
      "width = float(input(\"Enter the width of the rectangle: \"))\n",
      "\n",
      "# Calculate and print the area\n",
      "area = calculate_area(length, width)\n",
      "print(\"The area of the rectangle is\", area)\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "This program calcul\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at the end of the file explaining what the code does.\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```python\n",
      "# This program calculates the area of a rectangle\n",
      "\n",
      "# Function to calculate area\n",
      "def calculate_area(length, width):\n",
      "    return length * width\n",
      "\n",
      "# Input length and width\n",
      "length = float(input(\"Enter the length of the rectangle: \"))\n",
      "width = float(input(\"Enter the width of the rectangle: \"))\n",
      "\n",
      "# Calculate and print the area\n",
      "area = calculate_area(length, width)\n",
      "print(\"The area of the rectangle is\", area)\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "This program calcul\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 14. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement.\n",
      "````\n",
      "\n",
      "### Solution\n",
      "\n",
      "```python\n",
      "def greet_user(name):\n",
      "    \"\"\"\n",
      "    This function greets a person with their name.\n",
      "    \"\"\"\n",
      "    return \"Hello, \" + name + \"!\"  # This is a return statement\n",
      "\n",
      "print(greet_user(\"John\"))\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "\n",
      "The function `greet_user` takes a string `name` as an argument and returns a string that greets the person with their name. The return statement is followed by a single comment that explains what the return statement does.\n",
      "\n",
      "## Problem 2\n",
      "\n",
      "### Problem\n",
      "\n",
      "``\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement.\n",
      "````\n",
      "\n",
      "### Solution\n",
      "\n",
      "```python\n",
      "def greet_user(name):\n",
      "    \"\"\"\n",
      "    This function greets a person with their name.\n",
      "    \"\"\"\n",
      "    return \"Hello, \" + name + \"!\"  # This is a return statement\n",
      "\n",
      "print(greet_user(\"John\"))\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "\n",
      "The function `greet_user` takes a string `name` as an argument and returns a string that greets the person with their name. The return statement is followed by a single comment that explains what the return statement does.\n",
      "\n",
      "## Problem 2\n",
      "\n",
      "### Problem\n",
      "\n",
      "``\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 15. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement.\n",
      "````\n",
      "\n",
      "### Solution\n",
      "\n",
      "```python\n",
      "def greet_user(name):\n",
      "    \"\"\"\n",
      "    This function greets a person with their name.\n",
      "    \"\"\"\n",
      "    return \"Hello, \" + name + \"!\"  # This is a return statement\n",
      "\n",
      "print(greet_user(\"John\"))\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "\n",
      "The function `greet_user` takes a string `name` as an argument and returns a string that greets the person with their name. The return statement is followed by a single comment that explains what the return statement does.\n",
      "\n",
      "## Problem 2\n",
      "\n",
      "### Problem\n",
      "\n",
      "``\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement.\n",
      "````\n",
      "\n",
      "### Solution\n",
      "\n",
      "```python\n",
      "def greet_user(name):\n",
      "    \"\"\"\n",
      "    This function greets a person with their name.\n",
      "    \"\"\"\n",
      "    return \"Hello, \" + name + \"!\"  # This is a return statement\n",
      "\n",
      "print(greet_user(\"John\"))\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "\n",
      "The function `greet_user` takes a string `name` as an argument and returns a string that greets the person with their name. The return statement is followed by a single comment that explains what the return statement does.\n",
      "\n",
      "## Problem 2\n",
      "\n",
      "### Problem\n",
      "\n",
      "``\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 16. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "````\n",
      "\n",
      "## Code\n",
      "\n",
      "```python\n",
      "def add_one(number):\n",
      "    \"\"\"\n",
      "    This function adds one to the input number\n",
      "    \"\"\"\n",
      "    return number + 1  # This line adds one to the input number\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The function `add_one` takes an input `number` and returns the result of adding one to the input number. The comment at the return statement explains what the function does.\n",
      "\n",
      "## Test\n",
      "\n",
      "```python\n",
      "print(add_one(5))  # Output: 6\n",
      "```\n",
      "\n",
      "## Solution\n",
      "\n",
      "```python\n",
      "def add_one(number\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "````\n",
      "\n",
      "## Code\n",
      "\n",
      "```python\n",
      "def add_one(number):\n",
      "    \"\"\"\n",
      "    This function adds one to the input number\n",
      "    \"\"\"\n",
      "    return number + 1  # This line adds one to the input number\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The function `add_one` takes an input `number` and returns the result of adding one to the input number. The comment at the return statement explains what the function does.\n",
      "\n",
      "## Test\n",
      "\n",
      "```python\n",
      "print(add_one(5))  # Output: 6\n",
      "```\n",
      "\n",
      "## Solution\n",
      "\n",
      "```python\n",
      "def add_one(number\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 17. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```python\n",
      "def function_name(arg1, arg2):\n",
      "    # Your code here\n",
      "    return\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```python\n",
      "def function_name(arg1, arg2):\n",
      "    # Your code here\n",
      "    return arg1 + arg2  # Return the sum of arg1 and arg2\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The comment `# Your code here` is a placeholder for your own code. You can replace it with your own comments or code. The `return` statement is used to return a value from a function. In this case, it returns the sum of `arg\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```python\n",
      "def function_name(arg1, arg2):\n",
      "    # Your code here\n",
      "    return\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```python\n",
      "def function_name(arg1, arg2):\n",
      "    # Your code here\n",
      "    return arg1 + arg2  # Return the sum of arg1 and arg2\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The comment `# Your code here` is a placeholder for your own code. You can replace it with your own comments or code. The `return` statement is used to return a value from a function. In this case, it returns the sum of `arg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 18. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```python\n",
      "def function_name(arg1, arg2):\n",
      "    # Your code here\n",
      "    return\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```python\n",
      "def function_name(arg1, arg2):\n",
      "    # Your code here\n",
      "    return arg1 + arg2  # Return the sum of arg1 and arg2\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The comment `# Your code here` is a placeholder for your own code. You can replace it with your own comments or code. The `return` statement is used to return a value from a function. In this case, it returns the sum of `arg\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```python\n",
      "def function_name(arg1, arg2):\n",
      "    # Your code here\n",
      "    return\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```python\n",
      "def function_name(arg1, arg2):\n",
      "    # Your code here\n",
      "    return arg1 + arg2  # Return the sum of arg1 and arg2\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The comment `# Your code here` is a placeholder for your own code. You can replace it with your own comments or code. The `return` statement is used to return a value from a function. In this case, it returns the sum of `arg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 20. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def function_name(arg1, arg2):\n",
      "    # This is a comment\n",
      "    return arg1 + arg2\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```py\n",
      "def function_name(arg1, arg2):\n",
      "    # This function adds two numbers and returns the result\n",
      "    return arg1 + arg2\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "A comment in Python starts with a `#` symbol. It is used to explain the code and is ignored by the Python interpreter. In this case, the comment explains what the function does: it adds two numbers and returns the result.\n",
      "\n",
      "##\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def function_name(arg1, arg2):\n",
      "    # This is a comment\n",
      "    return arg1 + arg2\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```py\n",
      "def function_name(arg1, arg2):\n",
      "    # This function adds two numbers and returns the result\n",
      "    return arg1 + arg2\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "A comment in Python starts with a `#` symbol. It is used to explain the code and is ignored by the Python interpreter. In this case, the comment explains what the function does: it adds two numbers and returns the result.\n",
      "\n",
      "##\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 21. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def function_name(arg1, arg2):\n",
      "    # This is a comment\n",
      "    return arg1 + arg2\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```py\n",
      "def function_name(arg1, arg2):\n",
      "    # This function adds two numbers and returns the result\n",
      "    return arg1 + arg2\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "A comment in Python starts with a `#` symbol. It is used to explain the code and is ignored by the Python interpreter. In this case, the comment explains what the function does: it adds two numbers and returns the result.\n",
      "\n",
      "##\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def function_name(arg1, arg2):\n",
      "    # This is a comment\n",
      "    return arg1 + arg2\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```py\n",
      "def function_name(arg1, arg2):\n",
      "    # This function adds two numbers and returns the result\n",
      "    return arg1 + arg2\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "A comment in Python starts with a `#` symbol. It is used to explain the code and is ignored by the Python interpreter. In this case, the comment explains what the function does: it adds two numbers and returns the result.\n",
      "\n",
      "##\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 22. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_statement():\n",
      "    return 10  # This is a return statement\n",
      "```\n",
      "````\n",
      "\n",
      "## Code\n",
      "\n",
      "```py\n",
      "def return_statement():\n",
      "    return 10  # This is a return statement\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The function `return_statement` is defined to return the integer `10`. The comment `# This is a return statement` is added at the end of the return statement to explain its purpose.\n",
      "\n",
      "## Test\n",
      "\n",
      "```py\n",
      "def test_return_statement():\n",
      "    assert return_statement() == 10\n",
      "```\n",
      "\n",
      "This test\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_statement():\n",
      "    return 10  # This is a return statement\n",
      "```\n",
      "````\n",
      "\n",
      "## Code\n",
      "\n",
      "```py\n",
      "def return_statement():\n",
      "    return 10  # This is a return statement\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The function `return_statement` is defined to return the integer `10`. The comment `# This is a return statement` is added at the end of the return statement to explain its purpose.\n",
      "\n",
      "## Test\n",
      "\n",
      "```py\n",
      "def test_return_statement():\n",
      "    assert return_statement() == 10\n",
      "```\n",
      "\n",
      "This test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 23. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_statement():\n",
      "    return 10  # This is a return statement\n",
      "```\n",
      "````\n",
      "\n",
      "## Code\n",
      "\n",
      "```py\n",
      "def return_statement():\n",
      "    return 10  # This is a return statement\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The function `return_statement` is defined to return the integer `10`. The comment `# This is a return statement` is added at the end of the return statement to explain its purpose.\n",
      "\n",
      "## Test\n",
      "\n",
      "```py\n",
      "def test_return_statement():\n",
      "    assert return_statement() == 10\n",
      "```\n",
      "\n",
      "This test\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_statement():\n",
      "    return 10  # This is a return statement\n",
      "```\n",
      "````\n",
      "\n",
      "## Code\n",
      "\n",
      "```py\n",
      "def return_statement():\n",
      "    return 10  # This is a return statement\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The function `return_statement` is defined to return the integer `10`. The comment `# This is a return statement` is added at the end of the return statement to explain its purpose.\n",
      "\n",
      "## Test\n",
      "\n",
      "```py\n",
      "def test_return_statement():\n",
      "    assert return_statement() == 10\n",
      "```\n",
      "\n",
      "This test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 24. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tuple():\n",
      "    return (1, 2, 3)  # This function returns a tuple\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```py\n",
      "def return_tuple():\n",
      "    return (1, 2, 3)  # This function returns a tuple\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The comment is added at the end of the line where the tuple is returned. It explains what the function does.\n",
      "\n",
      "## Code\n",
      "\n",
      "```py\n",
      "def return_tuple():\n",
      "    return (1, 2, 3)  # This function returns a tuple\n",
      "```\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tuple():\n",
      "    return (1, 2, 3)  # This function returns a tuple\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```py\n",
      "def return_tuple():\n",
      "    return (1, 2, 3)  # This function returns a tuple\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The comment is added at the end of the line where the tuple is returned. It explains what the function does.\n",
      "\n",
      "## Code\n",
      "\n",
      "```py\n",
      "def return_tuple():\n",
      "    return (1, 2, 3)  # This function returns a tuple\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 25. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tens_place(num):\n",
      "    return num // 10 % 10\n",
      "```\n",
      "````\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "```py\n",
      "def return_tens_place(num):\n",
      "    # This function returns the tens place digit of a given number\n",
      "    return num // 10 % 10\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "The function `return_tens_place(num)` takes an integer `num` as input. It returns the tens place digit of `num`. The tens place digit is the digit that is in the 10\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tens_place(num):\n",
      "    return num // 10 % 10\n",
      "```\n",
      "````\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "```py\n",
      "def return_tens_place(num):\n",
      "    # This function returns the tens place digit of a given number\n",
      "    return num // 10 % 10\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "The function `return_tens_place(num)` takes an integer `num` as input. It returns the tens place digit of `num`. The tens place digit is the digit that is in the 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 26. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(x):\n",
      "    return x*10\n",
      "```\n",
      "````\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "```py\n",
      "def return_tensors(x):\n",
      "    # This function returns the input value multiplied by 10\n",
      "    return x*10\n",
      "```\n",
      "\n",
      "## Question 2\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(x):\n",
      "    return x*10\n",
      "```\n",
      "````\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "```py\n",
      "def return_tensors(x\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(x):\n",
      "    return x*10\n",
      "```\n",
      "````\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "```py\n",
      "def return_tensors(x):\n",
      "    # This function returns the input value multiplied by 10\n",
      "    return x*10\n",
      "```\n",
      "\n",
      "## Question 2\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(x):\n",
      "    return x*10\n",
      "```\n",
      "````\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "```py\n",
      "def return_tensors(x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 27. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(x):\n",
      "    return x*10\n",
      "```\n",
      "````\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "```py\n",
      "def return_tensors(x):\n",
      "    # This function returns the input value multiplied by 10\n",
      "    return x*10\n",
      "```\n",
      "\n",
      "## Question 2\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(x):\n",
      "    return x*10\n",
      "```\n",
      "````\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "```py\n",
      "def return_tensors(x\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(x):\n",
      "    return x*10\n",
      "```\n",
      "````\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "```py\n",
      "def return_tensors(x):\n",
      "    # This function returns the input value multiplied by 10\n",
      "    return x*10\n",
      "```\n",
      "\n",
      "## Question 2\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(x):\n",
      "    return x*10\n",
      "```\n",
      "````\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "```py\n",
      "def return_tensors(x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 28. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(input_tensor):\n",
      "    return input_tensor * 10  # This line multiplies the input tensor by 10\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```py\n",
      "def return_tensors(input_tensor):\n",
      "    return input_tensor * 10  # This line multiplies the input tensor by 10\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The comment is added at the end of the line where the tensor is multiplied by 10. This is a simple way to explain what the code does.\n",
      "\n",
      "## References\n",
      "\n",
      "-\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(input_tensor):\n",
      "    return input_tensor * 10  # This line multiplies the input tensor by 10\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```py\n",
      "def return_tensors(input_tensor):\n",
      "    return input_tensor * 10  # This line multiplies the input tensor by 10\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The comment is added at the end of the line where the tensor is multiplied by 10. This is a simple way to explain what the code does.\n",
      "\n",
      "## References\n",
      "\n",
      "-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 29. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(input: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    This function takes an input tensor and returns it.\n",
      "    \"\"\"\n",
      "    return input  # Return the input tensor\n",
      "````\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(input: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    This function takes an input tensor and returns it.\n",
      "    \"\"\"\n",
      "    return input  # Return the input tensor\n",
      "````\n",
      "\n",
      "````txt\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(input: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    This function takes an input tensor and returns it.\n",
      "    \"\"\"\n",
      "    return input  # Return the input tensor\n",
      "````\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(input: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    This function takes an input tensor and returns it.\n",
      "    \"\"\"\n",
      "    return input  # Return the input tensor\n",
      "````\n",
      "\n",
      "````txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 30. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # Add a comment here\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # This function returns the input string\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The function `return_tensors` takes a string `input` as an argument. The function returns the `input` string. The comment `# This function returns the input string` is a single line comment at the return statement. It explains what the function does.\n",
      "\n",
      "## Test\n",
      "\n",
      "```py\n",
      "print(\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # Add a comment here\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # This function returns the input string\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The function `return_tensors` takes a string `input` as an argument. The function returns the `input` string. The comment `# This function returns the input string` is a single line comment at the return statement. It explains what the function does.\n",
      "\n",
      "## Test\n",
      "\n",
      "```py\n",
      "print(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 31. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # Add a comment here\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # This function returns the input string\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The function `return_tensors` takes a string `input` as an argument. The function returns the `input` string. The comment `# This function returns the input string` is a single line comment at the return statement. It explains what the function does.\n",
      "\n",
      "## Test\n",
      "\n",
      "```py\n",
      "print(\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # Add a comment here\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # This function returns the input string\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The function `return_tensors` takes a string `input` as an argument. The function returns the `input` string. The comment `# This function returns the input string` is a single line comment at the return statement. It explains what the function does.\n",
      "\n",
      "## Test\n",
      "\n",
      "```py\n",
      "print(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 32. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # Add a comment here\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # This function returns the input string\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The function `return_tensors` takes a string `input` as an argument. The function returns the `input` string. The comment `# This function returns the input string` is a single line comment at the return statement. It explains what the function does.\n",
      "\n",
      "## Test\n",
      "\n",
      "```py\n",
      "print(\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # Add a comment here\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # This function returns the input string\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The function `return_tensors` takes a string `input` as an argument. The function returns the `input` string. The comment `# This function returns the input string` is a single line comment at the return statement. It explains what the function does.\n",
      "\n",
      "## Test\n",
      "\n",
      "```py\n",
      "print(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 33. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # Add a comment here\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # This function returns the input string\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The function `return_tensors` takes a string `input` as an argument. The function returns the `input` string. The comment `# This function returns the input string` is a single line comment at the return statement. It explains what the function does.\n",
      "\n",
      "## Test\n",
      "\n",
      "```py\n",
      "print(\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # Add a comment here\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # This function returns the input string\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The function `return_tensors` takes a string `input` as an argument. The function returns the `input` string. The comment `# This function returns the input string` is a single line comment at the return statement. It explains what the function does.\n",
      "\n",
      "## Test\n",
      "\n",
      "```py\n",
      "print(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 34. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # Add a comment here\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # This function returns the input string\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The function `return_tensors` takes a string `input` as an argument. The function returns the `input` string. The comment `# This function returns the input string` is a single line comment at the return statement. It explains what the function does.\n",
      "\n",
      "## Test\n",
      "\n",
      "```py\n",
      "print(\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # Add a comment here\n",
      "```\n",
      "````\n",
      "\n",
      "## Solution\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return input # This function returns the input string\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The function `return_tensors` takes a string `input` as an argument. The function returns the `input` string. The comment `# This function returns the input string` is a single line comment at the return statement. It explains what the function does.\n",
      "\n",
      "## Test\n",
      "\n",
      "```py\n",
      "print(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch at token 35. Generated text: \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return torch.tensor(input)\n",
      "```\n",
      "````\n",
      "\n",
      "**Code:**\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    # Converting string to tensor\n",
      "    return torch.tensor(input)\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "\n",
      "```txt\n",
      "RuntimeError: string argument without a codec\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "The error occurs because `torch.tensor()` function expects a sequence of numbers, not a string. If you want to convert a string to a tensor, you\n",
      "\n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return torch.tensor(input)\n",
      "```\n",
      "````\n",
      "\n",
      "**Code:**\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    # Converting string to tensor\n",
      "    return torch.tensor(input)\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "\n",
      "```txt\n",
      "RuntimeError: string argument without a codec\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "The error occurs because `torch.tensor()` function expects a sequence of numbers, not a string. If you want to convert a string to a tensor, you\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m predicted_token_id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs[:, idx, :], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predicted_token_id \u001b[38;5;241m!=\u001b[39m draft_tokens[idx]\u001b[38;5;241m.\u001b[39mitem():\n\u001b[0;32m----> 9\u001b[0m     generated_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch at token \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Generated text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.11/site-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.11/site-packages/transformers/generation/utils.py:2982\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2979\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2982\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2985\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1189\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1186\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1001\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    989\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    990\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    991\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m         position_embeddings,\n\u001b[1;32m    999\u001b[0m     )\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1001\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:750\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    749\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 750\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    753\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:309\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    307\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_tokens=100\n",
    "generated_tokens = draft_tokens.clone().detach()\n",
    "\n",
    "for idx in range(len(draft_tokens)):\n",
    "    outputs = model(prompt_inputs['input_ids'])[0].to(device)\n",
    "    predicted_token_id = torch.argmax(outputs[:, idx, :], dim=-1).item()\n",
    "\n",
    "    if predicted_token_id != draft_tokens[idx].item():\n",
    "        generated_output = model.generate(\n",
    "            prompt_inputs['input_ids'][:, :idx+1],\n",
    "            max_length=len(prompt_inputs['input_ids'][0]) + max_tokens, \n",
    "            do_sample=False,  \n",
    "            temperature=0.0\n",
    "        )\n",
    "        \n",
    "        generated_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "        print(f\"Mismatch at token {idx}. Generated text: {generated_text}\")\n",
    "        print(generated_text)\n",
    "    \n",
    "print(tokenizer.decode(generated_tokens, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "19857ae4-bab6-4c89-ad2f-6e2a6f702921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    public function getId(): ?int\n",
      "    {\n",
      "        return $this->id;\n",
      "    }\n",
      "\n",
      "    public function getName(): ?string\n",
      "    {\n",
      "        return $this->name;\n",
      "    }\n",
      "\n",
      "    public\n"
     ]
    }
   ],
   "source": [
    "def speculative_edit(prompt: str, draft: str, max_tokens: int) -> str:\n",
    "    prompt_inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    draft_inputs = tokenizer(draft, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    draft_tokens = draft_inputs['input_ids'][0].to(device)\n",
    "    \n",
    "    generated_tokens = draft_tokens.clone().detach()  # Clone the draft as the base\n",
    "    \n",
    "    for idx in range(len(draft_tokens)):\n",
    "        outputs = model(prompt_inputs['input_ids'])[0].to(device)\n",
    "        predicted_token_id = torch.argmax(outputs[:, idx, :], dim=-1).item()\n",
    "\n",
    "        if predicted_token_id != draft_tokens[idx].item():\n",
    "            generated_output = model.generate(\n",
    "                prompt_inputs['input_ids'][:, :idx+1],\n",
    "                max_length=len(prompt_inputs['input_ids'][0]) + max_tokens, \n",
    "                do_sample=False,  \n",
    "                temperature=0.0\n",
    "            )\n",
    "            \n",
    "            generated_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "            return generated_text\n",
    "\n",
    "    return tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "# Example usage:\n",
    "print(speculative_edit(\"Add a comment\", \"def return_tensors(): return torch.randn(2, 4)\", 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85f2fb27-b632-486d-b1d8-c85addae4726",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def speculative_edit(prompt: str, draft: str, max_tokens: int) -> str:\n",
    "    prompt_inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    draft_inputs = tokenizer(draft, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    draft_tokens = draft_inputs['input_ids'][0].to(device)\n",
    "    \n",
    "    generated_tokens = draft_tokens.clone().detach()  # Clone the draft as the base\n",
    "    \n",
    "    for idx in range(len(draft_tokens)):\n",
    "        outputs = model(prompt_inputs['input_ids'])[0].to(device)\n",
    "        predicted_token_id = torch.argmax(outputs[:, idx, :], dim=-1).item()\n",
    "\n",
    "        if predicted_token_id != draft_tokens[idx].item():\n",
    "            generated_output = model.generate(\n",
    "                prompt_inputs['input_ids'][:, :idx+1],\n",
    "                max_length=len(prompt_inputs['input_ids'][0]) + max_tokens, \n",
    "                do_sample=False,  \n",
    "                temperature=0.0\n",
    "            )\n",
    "            \n",
    "            generated_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "            return generated_text\n",
    "\n",
    "    return tokenizer.decode(generated_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7986b7e6-34fb-4b1b-8278-c1741c4a9d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Edit Output: \n",
      " \n",
      "````txt\n",
      "Please add a single comment in code at return statement\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    return torch.randn((2,4))\n",
      "```\n",
      "\n",
      "```py\n",
      "````\n",
      "\n",
      "## Answer\n",
      "\n",
      "```py\n",
      "def return_tensors(input:str):\n",
      "    # This function returns a tensor of size (2,4)\n",
      "    return torch.randn((2,4))\n",
      "```\n",
      "\n",
      "## Explanation\n",
      "\n",
      "The comment added at the return statement provides a brief explanation of what the function does. This is useful for both the programmer who might be reading the code and for any automated documentation generation tools.\n",
      "\n",
      "##\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example Usage\n",
    "prompt = \"\"\"\n",
    "````txt\n",
    "Please add a single comment in code at return statement\n",
    "\n",
    "```py\n",
    "def return_tensors(input:str):\n",
    "    return torch.randn((2,4))\n",
    "```\n",
    "\n",
    "```py\n",
    "````\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Testing vanilla edit\n",
    "vanilla_output = vanilla_edit(prompt, max_tokens=100)\n",
    "print(\"Vanilla Edit Output: \\n\", vanilla_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e510356d-92ab-4068-9b67-653605b8e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "draft=\"\"\"\n",
    "Review the code below and add a comment on the return statement if it is missing:\n",
    "\n",
    "```py\n",
    "def return_tensors(input: str):\n",
    "    return torch.randn((2, 4))\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8199d48b-4045-483d-9a7a-6d0123895a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speculative Edit Output: \n",
      " \n",
      "    public function getId(): ?int\n",
      "    {\n",
      "        return $this->id;\n",
      "    }\n",
      "\n",
      "    public function getName(): ?string\n",
      "    {\n",
      "        return $this->name;\n",
      "    }\n",
      "\n",
      "    public function setName(string $name): self\n",
      "    {\n",
      "        $this->name = $name;\n",
      "\n",
      "        return $this;\n",
      "    }\n",
      "\n",
      "    public function getDescription(): ?string\n",
      "    {\n",
      "        return $this->description;\n",
      "    }\n",
      "\n",
      "    public function setDescription(string $description): self\n",
      "    {\n",
      "        $this->description = $description;\n",
      "\n",
      "        return $this;\n",
      "    }\n",
      "\n",
      "    public function getPrice(): ?float\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Testing speculative edit\n",
    "speculative_output = speculative_edit(prompt, draft, max_tokens=100)\n",
    "print(\"Speculative Edit Output: \\n\", speculative_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e962fe-ba1f-473d-bf94-5da8b03789f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 2.1.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
